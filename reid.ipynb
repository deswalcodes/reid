{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6911604,"sourceType":"datasetVersion","datasetId":3969343},{"sourceId":7025045,"sourceType":"datasetVersion","datasetId":4040013},{"sourceId":7041892,"sourceType":"datasetVersion","datasetId":4051601}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:18:51.554853Z","iopub.execute_input":"2023-11-29T12:18:51.555320Z","iopub.status.idle":"2023-11-29T12:19:02.547255Z","shell.execute_reply.started":"2023-11-29T12:18:51.555275Z","shell.execute_reply":"2023-11-29T12:19:02.545869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_dir = '/kaggle/input/partial-reid/Partial_REID/train'  \ntest_data_dir = '/kaggle/input/partial-reid/Partial_REID/test'    \ninput_shape = (150, 150, 3)\nbatch_size = 16\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    preprocessing_function=preprocess_input\n)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(150, 150),\n    batch_size=batch_size,\n    class_mode='categorical' \n)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:19:02.549412Z","iopub.execute_input":"2023-11-29T12:19:02.550062Z","iopub.status.idle":"2023-11-29T12:19:02.723409Z","shell.execute_reply.started":"2023-11-29T12:19:02.550025Z","shell.execute_reply":"2023-11-29T12:19:02.722502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_generator = test_datagen.flow_from_directory(\n    test_data_dir,\n    target_size=(150, 150),\n    batch_size=batch_size,\n    class_mode='categorical', \n    shuffle=False \n)\nnum_classes = train_generator.num_classes\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:19:02.724701Z","iopub.execute_input":"2023-11-29T12:19:02.725277Z","iopub.status.idle":"2023-11-29T12:19:02.770076Z","shell.execute_reply.started":"2023-11-29T12:19:02.725239Z","shell.execute_reply":"2023-11-29T12:19:02.768845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax')) \n\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:19:02.773053Z","iopub.execute_input":"2023-11-29T12:19:02.773495Z","iopub.status.idle":"2023-11-29T12:19:03.245430Z","shell.execute_reply.started":"2023-11-29T12:19:02.773459Z","shell.execute_reply":"2023-11-29T12:19:03.244215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_generator, steps_per_epoch=train_generator.n // batch_size, epochs=200)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:19:03.246804Z","iopub.execute_input":"2023-11-29T12:19:03.247136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation = model.evaluate(train_generator, steps=len(train_generator))\ntrain_loss, train_accuracy = evaluation[0], evaluation[1]\n\nprint(f'Train Accuracy: {train_accuracy * 100:.2f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation = model.evaluate(test_generator, steps=len(test_generator))\ntest_loss, test_accuracy = evaluation[0], evaluation[1]\n\nprint(f'Test Accuracy: {test_accuracy * 100:.2f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\ndef plot_person_images(person_id):\n    occluded_images = []\n    non_occluded_images = []\n    predictions = []\n    \n    for i in range(len(test_generator)):\n        batch = test_generator[i]\n        images, labels = batch\n        predicted_classes = model.predict(images)\n        \n        occluded_indices = np.where(labels[:, person_id] == 1)[0]\n        occluded_images.extend(images[occluded_indices])\n        non_occluded_indices = np.where(labels[:, person_id] == 0)[0]\n        non_occluded_images.extend(images[non_occluded_indices])\n        predictions.extend(np.argmax(predicted_classes, axis=1))\n        \n    total_images = len(occluded_images) + len(non_occluded_images)\n    cols = 10 \n    rows = math.ceil(total_images / cols)\n    \n    plt.figure(figsize=(20, 20))  \n    for i, img in enumerate(occluded_images):\n        plt.subplot(rows, cols, i + 1)\n        plt.imshow(img, interpolation='nearest', aspect='auto')  \n        plt.title('Occluded', fontsize=12, y=1.05) \n        plt.axis('off')\n    \n    for i, img in enumerate(non_occluded_images):\n        plt.subplot(rows, cols, len(occluded_images) + i + 1)\n        plt.imshow(img, interpolation='nearest', aspect='auto') \n        plt.title('Non-Occluded', fontsize=12, y=1.05) \n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\nrandom_person_id = np.random.randint(0, num_classes) \nplot_person_images(random_person_id)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef classify_and_display_image(image_filename, model):\n    img = image.load_img(image_filename, target_size=(150, 150))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = img_array / 255.0  \n    prediction = model.predict(img_array)\n    predicted_class = np.argmax(prediction)\n    \n    if predicted_class == 1:\n        title = 'OCCLUDED'\n    else:\n        title = 'NONOCCLUDED'\n    \n    plt.imshow(img)\n    plt.title(title)\n    plt.axis('off')\n    plt.show()\n\nimage_filename = '/kaggle/input/input-image/002_03.jpg' \nclassify_and_display_image(image_filename, model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_accuracy = []\nepochs = 100  \n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n \n    history = model.fit(train_generator, steps_per_epoch=train_generator.n // batch_size, epochs=1, verbose=1)\n    evaluation = model.evaluate(train_generator, steps=len(train_generator), verbose=0)\n    train_accuracy = evaluation[1]\n    training_accuracy.append(train_accuracy)\n    print(f'Training Accuracy for Epoch {epoch + 1}: {train_accuracy * 100:.2f}%')\n\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, epochs + 1), training_accuracy, label='Training Accuracy', marker='o')\nplt.title('Training Accuracy vs Number of Epochs')\nplt.xlabel('Number of Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('Occlusion_Classification_Dataset-1.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}


   

